{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b61eab-9c58-41e4-9245-50c1a43aced2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, load_metric\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorBoard\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_callbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubCallback, KerasMetricCallback\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AdamWeightDecay, TFT5ForConditionalGeneration, T5Tokenizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset, load_metric\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from transformers.keras_callbacks import PushToHubCallback, KerasMetricCallback\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, AdamWeightDecay, TFT5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796cdab-7877-44a5-9bca-b63326954a78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## <center>SOURCE:</center>\n",
    "<center>https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77697211-a5b4-4a4c-ab3d-765b7923f67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoga\\AppData\\Local\\Temp\\ipykernel_12740\\3245786012.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "D:\\anaconda3\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get dataset and remove column \"id\"\n",
    "dataset = load_dataset(\"cnn_dailymail\")\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0e19ea-9198-42f3-a2ce-c159887eda1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'article', 'highlights'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'article', 'highlights'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'article', 'highlights'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c8378b-7245-4850-bcf4-68992ff8c8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint =  \"google-t5/t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac0aed-8aeb-43f1-9fef-8b0c56b43848",
   "metadata": {},
   "source": [
    "Show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d570964-d984-4037-b458-3c6d86390da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329cfd4c-551a-4406-ac3c-5d439767a2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dfdba79af47d3ecdbf7828724c43bf03999ee5cd</td>\n",
       "      <td>The oldest known shell to have been engraved by an early human has been uncovered inÂ a Dutch museum collection, where it remained unnoticed since the 1930s. The ancient mollusc, originally discovered on the island of Java, is between 430,000 and 540,000-years-old, dating to the time Homo erectus inhabited the remote volcanic outpost. The engravings resemble the previously oldest-known etchings, which were created by either Neanderthals or modern humans around 100,000 years ago. The oldest known shell (top) to have been engraved by an early human has been uncovered in a Dutch museum collection, where it remained unnoticed since the 1930s. Pictured here is its polished edge (bottom) The shell has a polished edge and a zigzag pattern of engravings, suggesting it was used from cutting or scraping. If this is the case, it means our ancestors, Homo erectus, were more intelligent than we thought. The successful, long-lived species emerged out of Africa almost two million years ago, and were possibly the first early humans to live in hunter-gather societies they also used rafts to travel the oceans. 'It rewrites human history,' Dr Stephen Munro, a Australian National University paleoanthropologist who made the find, toldÂ Business Insider. 'It's evidence that Homo erectus exploited these aquatic food resources, and fits with other evidence that they probably foraged in and around water.' Geometric engravings are considered to be a sign of modern cognitive abilities, but the origins of such behaviour have been debated. The latest analysis of freshwater mollusc shell collected from the Trinil fossil site on Java in the 1890s imply Homo erectus was also capable of 'modern' behaviour. The shell has a polished edge and a zigzag pattern of engravings (shown), suggesting it was used from cutting or scraping. It may mean our ancestors, Homo erectus, were more intelligent than we thought . Geometric engravings are considered to be a sign of modern cognitive abilities, but the origins of such behaviour have been debated. 'It rewrites human history,' said Dr Stephen Munro, a paleoanthropologist at the Australian National University . The team found that Homo erectus were able to open the shells by drilling a hole through the shell with a shark's tooth. This damaged the muscles causing  the valves of the shell to open, so that the contents can be eaten. Dr Josephine Joordens, of Leiden University in The Netherlands, said: 'The manufacture of geometric engravings is generally interpreted as indicative of modern cognition and behaviour. 'Key questions in the debate on the origin of such behaviour are whether this innovation is restricted to Homo sapiens and whether it has a uniquely African origin. 'Here we report on a fossil freshwater shell assemblage from the main bone layer of Trinil - the type locality of Homo erectus discovered by Eugene Dubois in 1891. The engravings inside Gorham's Cave in Gibraltar are the first known examples of Neanderthal rock art . It may look like a cross, but lines scratched into a cave wall could be proof that Neanderthals were more intelligent and creative than previously thought. The cross-hatched engravings inside Gorham's Cave in Gibraltar are the first known examples of Neanderthal rock art. The find is significant because it indicates that modern humans and their extinct cousins shared the capacity for abstract expression, according to a team of scientists who studied the site. Experts examined and described the grooves in a rock that had been covered with sediment in the study published in the PNAS journal. Archaeologists had previously found artefacts associated with Neanderthal culture laid over the top of the artwork, suggesting that the engravings must be older, said Professor Clive Finlayson, director of heritage at the Gibraltar Museum. 'The production of purposely made painted or engraved designs on cave walls is recognised as a major cognitive step in human evolution - considered exclusive to modern humans, he said. The latest analysis of the freshwater mollusc shell (shown) collected from the Trinil fossil site on Java in the 1890s imply Homo erectus was also capable of 'modern' behaviour . 'In the Dubois collection in the Natural History Museum in Leiden we found evidence for freshwater shellfish consumption by hominins, one unambiguous shell tool and a shell with a geometric engraving. 'Together our data indicate the engraving was made by Homo erectus and it is considerably older than the oldest geometric engravings described so far. 'Although it is at present not possible to assess the function or meaning of the engraved shell this discovery suggests that engraving abstract patterns was in the realm of Asian Homo erectus cognition and neuromotor control.' The ability to make and use tools dates back millions of years. Chimpanzees - our closest living relatives - can devise spear-like weapons for hunting and create specialised tool kits for foraging ants. This suggests our family tree could have possessed wooden tools since the ancestors of humans and chimps diverged some four million years ago. The team found that Homo erectus (artist's impression pictured) were able to open the shells by drilling a hole through the shell with a shark's tooth . The shell was originally found in Java, an island in Indonesia. Today it has a population of 143 million .</td>\n",
       "      <td>Mollusc, found in Java, is between 430,000 and 540,000-years old .\\nShell has a polished edge as well as a zigzag pattern of grooves .\\nThis suggests it was used as an ancient tool for cutting and scrapping .\\nEarliest previously known engravings are at least 300,000 years younger .\\nFind implies that Homo erectus was also capable of 'modern' behaviour .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21d0089f2195e86c247c2fa6882bc3f32b46cb39</td>\n",
       "      <td>London (CNN) -- The editor of an Italian magazine that has caused an uproar by publishing vacation photos of a bikini-clad Catherine, Duchess of Cambridge, with her baby bump visible accused the international media Wednesday of overreacting. Chi Editor-in-Chief Alfonso Signorini said the photos were not scandalous, do not \"wrong the image\" of the royal couple and \"portray a couple in love in a happy moment while they are walking on the beach.\" The photos were taken on a public beach and bought from an international agency, he said. \"We cannot talk about violation of privacy when we publish pictures of public people in a public place, out in the open as it is the case of a beach that is visited by other people,\" he said. St. James's Palace -- which represents Catherine, her husband, Prince William, and his brother, Harry -- responded with dismay Tuesday to news that the photos would be released. \"We are disappointed that photographs of the Duke and Duchess on a private holiday look likely to be published overseas,\" a palace representative said. \"This is a clear breach of the couple's right to privacy.\" Despite the royal family's objections, an Australian magazine said it too would be publishing the images. Woman's Day said it had won Australian publishing rights after two days of \"furious bidding\" between three magazines. \"It's ticking all the boxes, it's Kate who we love and adore, who's a fantastic cover girl, a great seller. It's a bikini shot, a beach shot, beautiful crystal blue water. And she's pregnant. It's the bump that we've been waiting to see for such a long time,\" Woman's Day editor Fiona Connolly told Australia's Ten News. It was not immediately clear when or exactly where the pictures were taken. British and U.S. media -- including People, like CNN a division of Time Warner -- reported that the royal couple recently vacationed on the secluded Caribbean isle of Mustique. Connolly said the images were taken at a public beach where the royal couple was mingling with other holidaymakers, describing the decision to publish them as \"very easy.\" She added: \"Australians are so much more laid back about seeing bikinis and beach shots than the British are. I think there's a bit of hypersensitivity over this particular set (of photos).\" UK newspapers did not reproduce the images but wrote of the royal couple's anguish about the breach of privacy. And broadcaster ITV issued an apology after its \"This Morning\" show \"accidentally showed an unblurred image of the magazine cover, which briefly showed the photographs. \"This was a deeply regrettable error and we are very sorry,\" it said. On Tuesday, Chi described the photos as \"extraordinary images of the Duke and Duchess of Cambridge during their dream holiday in the Caribbean.\" \"The future mum, now in her fourth month of pregnancy, wears a small bikini that enhances her now visible bump,\" the magazine said. The Duchess, whose maiden name is Kate Middleton, has kept a low profile since the announcement in December that she is set to give birth to her first child in July. Images showing any evidence of a baby bump have been hard to come by in that time. The Chi photos, though, are not her first encounter with paparazzi, which tracked her during her long courtship with William, their engagement and their time since April 2011 as a married couple. The highest-profile example came in September 2012, when the French magazine Closer ran photographs of Catherine privately sunbathing topless while on a holiday with William in France. Besides that magazine, some of those photos also were published in the Irish Daily Star newspaper and Chi, which according to its parent company Mondadori is a women's magazine with an average circulation of more than 218,000 and a readership well beyond that. Soon after the photos came out, the Duke and Duchess of Cambridge took legal action against Closer, which was fined by a French court and ordered not to distribute the edition in print or online. It was also told to hand over the photos to the royals. READ RELATED: The Duchess of Cambridge's fashion secrets revealed . CNN's Max Foster reported from Rome and Susannah Palk from London, and Laura Smith-Spark wrote the story from London. CNN's Gisella Deputato, Hada Messia and Greg Botelho contributed to this report.</td>\n",
       "      <td>Editor of the Italian magazine Chi defends its use of photos of the Duchess of Cambridge .\\nThe photos show her in \"a small bikini that enhances her now visible (baby) bump,\" Chi says .\\nA disappointed St. James's Palace says the pictures violate her privacy .\\nChi was among the publications to run photos of Catherine sunbathing topless last year .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2e3997c123360238997ddc359ef132c2cfaf719</td>\n",
       "      <td>Deyka Ayan Hassan said she was 'disgusted' with herself after her vile comment . A student who tweeted that people wearing Help for Heroes t-shirts 'deserved to be beheaded' after soldier Lee Rigby was killed was arrested after complaining to police about getting threatening replies, a court heard today. Deyka Ayan Hassan, 21, contacted officers after receiving hundreds of vitriolic responses to the message on May 22, including threats to rape her and kill her by burning down her home, Hendon Magistrates' Court heard. But she was herself later arrested at home after admitting to police she had tweeted 'to be honest, if you wear a Help for Heroes t-shirt you deserve to be beheaded' as a 'joke' about the design of the item of clothing. The English and politics undergraduate at Kingston University, who lives in Harrow, north west London, was ordered to do 250 hours of unpaid work by magistrates today, having admitted a charge of sending a malicious electronic message at an earlier hearing. Chairman of the bench Nigel Orton told her she could have been jailed for what she did but that magistrates accepted she hadnâ€™t known it was a soldier who had been killed when she posted it. 'The tragic events in Woolwich that day have created a context which made this tweet appear extreme,' he said. 'It had a huge impact and clearly caused offence and distress. We accept you didnâ€™t intend to cause harm and you felt it was a joke. Hassan, who was escorted to court with relatives and friends, meant her comment to be a jokey criticism of the design of the t-shirt worn by supporters of the forces charity, magistrates heard . 'Your act was naive and foolish and without regard to the general public at a time of heightened sensitivity.' The court heard that Hassanâ€™s father works in Somalia for charities including US Aid and Prevent, combating extremism. Magistrates accepted she hadnÂ¿t known it was a soldier who had been killed when she posted her comment . He is also a former 'associate advisor for policing diversity' to the Metropolitan Police. Hassan sat with her head bowed throughout this afternoonâ€™s hearing. The magistrates heard that she tweeted . the message at around 4pm on May 22, just hours after Drummer Rigby was . brutally hacked to death in south east London. At that time, various rumours were circulating via social media about the manner of the soldierâ€™s killing, including that he had been decapitated. Magistrates were told that she tweeted the message to her 600 followers after seeing other people making remarks about the case and wanting to join in. She meant it to be a jokey criticism of the design of the t-shirt worn by supporters of the forces charity, the court heard. But after being alerted by friends Hassan found the message has sparked angry replies, including some saying she deserved to be raped or killed. She immediately closed down her Twitter and Facebook accounts and then went to the local police station, where she was interviewed before being arrested for the original tweet. Katie Weiss, prosecuting, said: 'She said she was making a joke about the design of the (HfH) t-shirt. 'She said it was a comment she would (typically) make about clothes and shoes she didnâ€™t like. After being alerted by friends Hassan found the message has sparked angry replies, including some saying she deserved to be raped or killed, the court heard . Itpal Dhillon, defending, said Hassan admitted that what she wrote had caused 'gross offence' and she was disgusted with herself, having been brought up to be tolerant and respect society. 'At the time she did not know the full details relating to the horrific incident,' Ms Dhillon said. 'She certainly didnâ€™t know that the man who was killed was a soldier and the killing may be motivated by extremist beliefs or values. 'She accepts it (the tweet) was distasteful and disgusting.' Ms Dhillon said her client, works in a shoe shop alongside her studies. She also volunteers with charities including Crisis at Christmas and a local Somali community group. She had not intended any 'racist undertone' to the message, Ms Dhillon said.</td>\n",
       "      <td>Deyka Ayan Hassan, 21, meant the comment to be a jokey criticism of the design of the t-shirt worn by supporters of the forces charity .\\nShe was arrested at home after admitting to police she had tweeted 'to be honest, if you wear a Help for Heroes t-shirt you deserve to be beheaded'\\nOrdered to do 250 hours of unpaid work by magistrates today, having admitted a charge of sending a malicious electronic message .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02491c260d3087e6e2f53693ed17be56a716e8a4</td>\n",
       "      <td>(CNN) -- It was barely floating, perhaps minutes from sinking, a small skiff, washed up against an unforgiving cement breakwater. The flat-bottomed boat was homemade, cobbled together with no evident craftsmanship, from scarce and salvaged materials. It was about 14 feet long and 4 feet wide, with a hand-hewn pole for a mast and fallen sails that looked like burlap. As waves and a relentless tide drove it into the concrete wall, the boat was going under, surrendering to the ocean that had brought it to Key Largo, Florida. A small crowd had gathered. Some were taking pictures. My wife and I asked what was happening. One in the group told us that some men came over from Cuba in this boat the previous night. They had given themselves to the ocean and whatever fate their desperate journey might bring them. They had traveled nearly 200 miles, day and night, through rough and open waters, sailing this pitiful craft on pitiless seas. Local security had found them, walking around, lost. They had no destination beyond America. They were picked up and turned over to U.S. Customs and Border Protection. We asked how many there were. Someone in the group told us, \"Nine men risked their lives to come to the United States on this.\" What is exceptional about this story is that it is not exceptional. Immigrants constantly risk not only their own lives, but also their children's, for the economic opportunity we enjoy with indifference. While these men apparently were from Cuba, covered by the decades-old \"wet-foot-dry-foot\" Cuban immigration policy, a trickle of immigrants from Central America has suddenly become a flood of women and children, swamping our border security offices. Tens of thousands of Guatemalans and Ecuadorians have been drawn by the luster of an American economy we find tarnished. Unlike those nine Cuban men, however, these immigrants have an additional reason for coming: President Barack Obama invited them here. As Byron York explained in the Washington Examiner, \"President Obama's DACA decree -- Deferred Action for Childhood Arrivals, which allowed thousands of illegal immigrants to stay in this country if they came at a young age -- created, in effect, a magnet for young people to try to enter the U.S. illegally.\" This degree, announced in June 2012, allows immigrant children to stay if they had been in the United States continuously from June 2007 to June 2012. Some say, however, the decree has swept through Central America. According to The Washington Post, a recent leaked memo by Border Patrol agents spoke of this wave of new immigrants, saying they were motivated to come now, in this surge, because they had heard of the change in U.S. policy that would allow them to remain in America. Deferred Action for Childhood Arrivals must have sounded like popular politics to this President, yet it was cruelty disguised as kindness. Our President's incompetence has created a humanitarian crisis: He has lured an endless stream of children and mothers to risk everything to travel here. We are a country divided today. The prevailing debate is what is wrong with America and who among us should be blamed for it. Yesterday's newcomers blame today's arrivals. Those who have already walked through the golden door of opportunity would ungenerously close it behind them. We confront each other over what we lack, forgetting the greatness we have built together and how much better we could still be. Democrats demand too much of immigration reform. Republicans demand too little. The stalemate serves both their political ambitions. And as we watch men risking their lives on what can barely be called a boat and children desperately throwing themselves across our borders, we harbor a shame: No immigration reform will pass Congress this year. Perhaps both Democrats and Republicans should remember: Once, many of our families, in either this or a previous incarnation, came here as immigrants, seeking a better life and freedom. Imagine that you are one of the nine men on that boat. Or a muddied child in the Rio Grande. We may never be called upon to make those journeys, but what would each of us risk to live in the greatest country in human history? Much more, it appears, than our leaders would risk so we can keep living there. Join us on Facebook.com/CNNOpinion.</td>\n",
       "      <td>Alex Castellanos says a new wave of young immigrants were practically invited by Obama .\\nHe says a recent change in U.S. policy allows them to remain in America .\\nCastellanos: Yesterday's newcomers blame today's arrivals .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b51647061550f7b34560ebb63340d89ed61091d0</td>\n",
       "      <td>By . Daniel Martin . PUBLISHED: . 19:52 EST, 13 August 2013 . | . UPDATED: . 03:40 EST, 14 August 2013 . Aid minister Alan Duncan has urged David Cameron to avoid the â€˜tokenisticâ€™ promotion of women at the forthcoming Cabinet reshuffle. He claimed that it would be wrong to give women top departmental jobs at the expense of more talented or experienced men. Mr Duncan, Parliamentâ€™s first openly gay MP, said: â€˜I never wanted to be a token gay and now things have progressed so there is no need for it. Nobody should want to be a token woman.â€™ Write caption here . The Prime Minister is expected to shake his top team up in the autumn, with Tories such as Amber Rudd, Nicky Morgan and Karen Bradley tipped for jobs. Some male Conservative MPs are concerned that the drive to promote women means they will be overlooked. Talented: Mr Duncan's boss Justine Greening . Mr Duncan, a junior minister at the Department for International Development, said his own boss Justine Greening was proof that talented women could advance to the front row on their own. He told the Financial Times: â€˜When I look around DfID... thereâ€™s enormous scope for women and no bar to their success. â€˜There are moments when I think they run the place.â€™ Miss Greening was moved against her will at the last reshuffle in September 2012. She was previously Transport Secretary. Mr Duncan said she had settled into her new aid job. â€˜Sheâ€™s thrown herself into it and earned respect very quickly, is absolutely on top of things, and she delegates very well.â€™ He added: â€˜We started with three grey-haired Oxbridge men and now Iâ€™m the last man standing.â€™</td>\n",
       "      <td>The PM is expected to shake his top team up in the autumn .\\nAmber Rudd, Nicky Morgan and Karen Bradley are all tipped for jobs .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aad12b-b63b-496d-8ed7-a0e20f5d4064",
   "metadata": {},
   "source": [
    "If we are using one of the five T5 checkpoints we have to prefix the inputs with \"summarize:\" (the model can also translate and it needs the prefix to know which task it has to perform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d010c27-1a26-4b4c-b761-3e41283184a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a24f4a-2711-4463-b95e-6c070f175dc1",
   "metadata": {},
   "source": [
    "Write the function that will preprocess our samples. feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c50eefb-13b1-4b78-b0d5-4da431ef221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"highlights\"], max_length=max_target_length, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13fe5c9-dad7-4be3-97a1-df27b69b657c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[938, 3, 5, 3, 26138, 3373, 3, 5, 3, 10744, 8775, 20619, 2326, 10, 3, 5, 968, 10, 2596, 3, 6038, 6, 944, 1797, 2038, 3, 5, 1820, 3, 5, 3, 6880, 4296, 11430, 10, 3, 5, 627, 10, 3420, 3, 6038, 6, 944, 1797, 2038, 3, 5, 37, 25214, 13, 8, 5186, 839, 6502, 309, 25985, 16, 1117, 16711, 65, 6666, 6149, 3986, 13, 2078, 724, 16, 5186, 839, 6, 2698, 242, 157, 7, 11, 2549, 3540, 12, 8, 3, 88, 7768, 17, 159, 71, 6722, 16, 1480, 1600, 11, 778, 1797, 5, 37, 538, 1685, 1775, 65, 4683, 46, 18599, 13, 4773, 21, 1321, 113, 5526, 874, 9894, 11, 808, 30124, 5, 16098, 1079, 17796, 9, 41, 22665, 61, 13, 8, 5186, 839, 6502, 309, 25985, 16, 1117, 16711, 65, 6666, 6149, 3986, 13, 2078, 724, 16, 5186, 839, 6, 2698, 242, 157, 7, 11, 2549, 3540, 12, 8, 3, 88, 7768, 17, 159, 71, 3, 5, 1015, 18502, 1707, 2350, 3440, 11019, 120, 571, 3820, 845, 8, 1020, 19, 731, 6, 68, 4298, 473, 34, 31, 7, 359, 12, 5685, 151, 12, 8, 487, 4773, 5, 37, 3, 26, 25985, 2162, 30, 2089, 24, 16098, 1079, 17796, 9, 19, 838, 97, 326, 227, 271, 12223, 28, 3, 88, 7768, 17, 159, 71, 5, 37, 3, 26, 25985, 845, 3, 88, 25316, 8, 7952, 190, 3, 27288, 542, 298, 7078, 3, 9, 2542, 21, 6164, 3, 31917, 25214, 7, 16, 5308, 336, 847, 5, 3, 21828, 7, 13, 3, 88, 7768, 17, 159, 71, 560, 17055, 6, 7718, 655, 6, 1453, 13, 17699, 6, 25808, 11, 22827, 17294, 5, 5186, 839, 6502, 309, 25985, 16, 1117, 16711, 41, 22665, 61, 19, 213, 8, 25214, 19, 1069, 3, 5, 1], [41, 254, 17235, 61, 1636, 21171, 5708, 9, 47, 46, 3224, 12748, 4618, 324, 288, 21, 8, 8327, 18, 308, 9, 221, 5076, 1775, 6, 464, 16, 8, 4889, 24, 9127, 7, 20265, 13, 1786, 26, 32, 53, 57, 7326, 7, 5, 17857, 8, 828, 6, 5779, 1854, 397, 24, 8, 3479, 18, 1201, 18, 1490, 307, 715, 5502, 1279, 28, 3, 9, 2672, 2117, 1765, 1470, 12, 199, 515, 3, 9, 7738, 5944, 11, 129, 13731, 5, 71, 4336, 10394, 1149, 15, 9, 1361, 16, 412, 5, 134, 5, 3570, 2243, 16, 368, 5092, 2818, 17316, 15, 7, 5708, 9, 6, 92, 801, 38, 96, 634, 18389, 1140, 976, 13, 338, 112, 1075, 38, 3, 9, 2095, 5502, 12, 199, 8, 2672, 2117, 1765, 1470, 16, 2509, 21, 540, 11, 5504, 6, 379, 3, 9, 2158, 109, 226, 1605, 5, 86, 80, 3421, 6, 8, 10394, 1854, 2897, 6, 5708, 9, 3, 9741, 12, 726, 192, 38, 7, 6500, 29, 7, 12, 5781, 8374, 2672, 15939, 5, 37, 14804, 7, 133, 7663, 38, 7326, 7, 6, 12473, 147, 70, 8874, 274, 5262, 135, 6, 1315, 12, 8, 10394, 5, 96, 19494, 6, 8, 41, 17939, 257, 61, 1500, 59, 12, 888, 1039, 28, 8, 7738, 5944, 6, 68, 5708, 9, 341, 1204, 3, 9, 1942, 21, 1898, 95, 8, 4677, 976, 2822, 3, 29905, 243, 16, 3, 9, 2493, 5, 37, 10394, 92, 1854, 2897, 24, 5708, 9, 261, 112, 2095, 13402, 12, 1242, 7749, 21, 2672, 2117, 11758, 5, 5708, 9, 6, 1315, 12, 8, 10394, 6, 258, 261, 11463, 44, 8, 3761, 12, 1855, 8, 7749, 16, 112, 2331, 18, 106, 17641, 30, 6846, 45, 8327, 12, 8, 19169, 152, 5750, 5, 2243, 2691, 1883, 57, 16273, 7, 103, 59, 11610, 8, 564, 13, 8, 2672, 2117, 1765, 1470, 28, 84, 5708, 9, 3, 18280, 29105, 15, 26, 68, 845, 8, 1470, 65, 118, 3, 31356, 3, 29, 4667, 9798, 7, 45, 1747, 224, 38, 25101, 11, 8, 19169, 152, 5750, 57, 17810, 135, 96, 77, 1583, 3365, 10827, 3, 6443, 23590, 7, 13, 1759, 6, 379, 13634, 7, 535, 37, 1470, 96, 10293, 118, 3, 26962, 3, 29, 4667, 9798, 7, 16, 368, 5092, 11, 8975, 976, 8, 10394, 845, 5, 30336, 10195, 5708, 9, 30, 2818, 16, 8327, 15934, 6, 2599, 5, 94, 47, 59, 2017, 964, 823, 5708, 9, 65, 46, 4917, 6, 11, 2095, 4298, 228, 59, 36, 2017, 3495, 21, 1670, 5, 5708, 9, 65, 1279, 21, 8, 8327, 18, 308, 9, 221, 5076, 1775, 437, 9047, 6, 379, 3, 26243, 17032, 16, 8327, 15934, 11, 464, 38, 3, 9, 4618, 324, 288, 16, 8, 480, 7141, 1745, 44, 8327, 1331, 5735, 6, 1315, 12, 8, 10394, 5, 1541, 1332, 8693, 3, 88, 141, 118, 464, 16, 8, 3224, 12748, 4889, 5, 5708, 9, 8519, 3991, 13, 3052, 53, 11, 3, 9, 8805, 53, 3, 9, 25662, 12, 10973, 29309, 6, 29105, 53, 12, 10973, 29309, 11, 7007, 16, 3, 14356, 6413, 16, 785, 3, 9942, 45, 7173, 25487, 1756, 5, 216, 19, 5018, 12, 2385, 16, 2822, 1614, 16, 2599, 30, 2875, 5, 156, 3, 21217, 6, 5708, 9, 228, 522, 280, 16, 5714, 5, 19602, 31, 7, 31054, 10131, 235, 9859, 12, 48, 934, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[16098, 1079, 17796, 9, 6, 13, 1117, 16711, 6, 19, 838, 97, 326, 227, 271, 12223, 3, 5, 216, 25316, 8, 7952, 190, 3, 27288, 542, 16, 5308, 3, 5, 2345, 724, 16, 5186, 839, 6, 2698, 242, 157, 7, 11, 2549, 3540, 228, 43, 118, 6666, 3, 5, 1], [20383, 10394, 10, 10400, 261, 112, 1075, 12, 199, 29309, 2117, 11758, 3, 5, 21171, 5708, 9, 6, 46, 3224, 12748, 4618, 324, 288, 6, 3, 18280, 2139, 563, 129, 13731, 3, 5, 216, 92, 3, 9741, 12, 726, 192, 38, 7, 6500, 29, 7, 16, 3, 9, 7738, 5944, 6, 3, 9, 10394, 1854, 2897, 3, 5, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(dataset[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed35de4-83d1-43d8-b342-43fd65c89373",
   "metadata": {},
   "source": [
    "To apply this function on all the pairs of sentences in the dataset,  use the `map` method of the `dataset` object  created earlier. This will apply the function on all the elements of all the splits in `dataset`, so the training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d413d72-4ad3-471a-9a7e-69bc213adb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 287113/287113 [11:05<00:00, 431.75 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13368/13368 [00:29<00:00, 454.45 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11490/11490 [00:25<00:00, 449.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5deea49-7372-429b-a21d-f93ee3859f66",
   "metadata": {},
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is sequence-to-sequence (both the input and output are text sequences), we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc24a1c-fde5-48ea-ba42-e52b0b081acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFT5ForConditionalGeneration.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0010c0-ca13-4630-aeee-bc55c54e004c",
   "metadata": {},
   "source": [
    "We set some parameters like the `learning rate` and the `batch_siz` eand customize the `weight decay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8d63620-2ece-41d5-8c7b-6dfd59263079",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210312c-8134-48b2-a049-a2338bfc306b",
   "metadata": {},
   "source": [
    "We need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Note that our data collators are designed to work for multiple frameworks, so ensure you set the `return_tensors='np'` argument to get NumPy arrays out, our TF dataset pipeline actually uses a NumPy loader internally, which is wrapped at the end with a `tf.data.Dataset`. As a result, `np` is usually more reliable and performant when you're using it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eae61ad0-d407-490b-9e8a-7150333c49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a076e740-991d-4314-9ae0-3d911c36a1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'article', 'highlights', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 287113\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa387725-133d-492a-bfbe-c9800902d887",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to `tf.data.Dataset`, which Keras understands natively. Using [`Model.prepare_tf_dataset()`](https://huggingface.co/docs/transformers/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset). The `Model` method can inspect the model to determine which column names it can use as input, which means we don't need to specify them yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2577f8f3-5fb1-434a-b9bf-350db2162df7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprepare_tf_dataset(\n\u001b[0;32m      9\u001b[0m     tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     11\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1429\u001b[0m, in \u001b[0;36mTFPreTrainedModel.prepare_tf_dataset\u001b[1;34m(self, dataset, batch_size, shuffle, tokenizer, collate_fn, collate_fn_args, drop_remainder, prefetch)\u001b[0m\n\u001b[0;32m   1427\u001b[0m model_labels \u001b[38;5;241m=\u001b[39m find_labels(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcols_to_retain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(inspect\u001b[38;5;241m.\u001b[39msignature(dataset\u001b[38;5;241m.\u001b[39m_get_output_signature)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m-> 1429\u001b[0m     output_signature, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_output_signature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate_fn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcols_to_retain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1437\u001b[0m     \u001b[38;5;66;03m# TODO Matt: This is a workaround for older versions of datasets that are missing the `cols_to_retain`\u001b[39;00m\n\u001b[0;32m   1438\u001b[0m     \u001b[38;5;66;03m#            argument. We should remove this once the minimum supported version of datasets is > 2.3.2\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m     unwanted_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1440\u001b[0m         feature\n\u001b[0;32m   1441\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[0;32m   1442\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_inputs \u001b[38;5;129;01mand\u001b[39;00m feature \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1443\u001b[0m     ]\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:280\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin._get_output_signature\u001b[1;34m(dataset, collate_fn, collate_fn_args, cols_to_retain, batch_size, num_test_batches)\u001b[0m\n\u001b[0;32m    278\u001b[0m         test_batch \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m test_batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m cols_to_retain}\n\u001b[0;32m    279\u001b[0m     test_batch \u001b[38;5;241m=\u001b[39m [{key: value[i] \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m test_batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(test_batch_size)]\n\u001b[1;32m--> 280\u001b[0m     test_batch \u001b[38;5;241m=\u001b[39m collate_fn(test_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcollate_fn_args)\n\u001b[0;32m    281\u001b[0m     test_batches\u001b[38;5;241m.\u001b[39mappend(test_batch)\n\u001b[0;32m    283\u001b[0m tf_columns_to_signatures \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:612\u001b[0m, in \u001b[0;36mDataCollatorForSeq2Seq.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m             feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([remainder, feature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m--> 612\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;66;03m# prepare decoder_input_ids\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    623\u001b[0m     labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_decoder_input_ids_from_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    626\u001b[0m ):\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3369\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3366\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3367\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m-> 3369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:224\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    220\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:713\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tensor_type \u001b[38;5;241m==\u001b[39m TensorType\u001b[38;5;241m.\u001b[39mPYTORCH:\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 713\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     is_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_tensor\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de480ed-56ec-4268-b386-b24f43ef927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c9f8c01-6d2f-43db-bbc5-22bfbd9bcf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_predictions = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    # Add mean generated length\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8afb905-27d6-4560-ac1a-f4c18445684b",
   "metadata": {},
   "source": [
    "The callback takes two main arguments - a `metric_fn` and an `eval_dataset`. It then iterates over the `eval_dataset` and collects the model's outputs for each sample, before passing the `list` of predictions and the associated `list` of labels to the user-defined `metric_fn`. If the `predict_with_generate` argument is `True`, then it will call `model.generate()` for each input sample instead of `model.predict()` - this is useful for metrics that expect generated text from the model, like `ROUGE`.\r\n",
    "\r\n",
    "This callback allows complex metrics to be computed each epoch that would not function as a standard Keras Metric. Metric values are printed each epoch, and can be used by other callbacks like `TensorBoard` or `EarlyStopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87427a4-eaec-4fd9-97b4-86b9144d8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = TensorBoard(log_dir=\"./summarization_model_save/logs\")\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"./summarization_model_save\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_model_id=push_to_hub_model_id,\n",
    ")\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True, use_xla_generation=True\n",
    ")\n",
    "\n",
    "callbacks = [metric_callback, tensorboard_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e418934-68ec-4ff2-bd0b-85e896cd7150",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1571, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1170\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1169\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filei8djsozj.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1571\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss \u001b[38;5;129;01mand\u001b[39;00m parse(tf\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1569\u001b[0m     \u001b[38;5;66;03m# Newer TF train steps leave this out\u001b[39;00m\n\u001b[0;32m   1570\u001b[0m     data \u001b[38;5;241m=\u001b[39m expand_1d(data)\n\u001b[1;32m-> 1571\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_x_y_sample_weight\u001b[49m(data)\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;66;03m# If the inputs are mutable dictionaries, make a shallow copy of them because we will modify\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;66;03m# them during input/label pre-processing. This avoids surprising the user by wrecking their data.\u001b[39;00m\n\u001b[0;32m   1574\u001b[0m \u001b[38;5;66;03m# In addition, modifying mutable Python inputs makes XLA compilation impossible.\u001b[39;00m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1571, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=validation_dataset, \n",
    "    epochs=1, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc3042-5c9b-4753-be1e-0774906cbbee",
   "metadata": {},
   "source": [
    "## <center>SOURCE:</center>\n",
    "<center>https://huggingface.co/docs/transformers/tasks/summarization</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd73599-628c-40a0-b9d1-8e320f12d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset and remove column \"id\"\n",
    "dataset = load_dataset(\"cnn_dailymail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbed241-c0ff-4f9f-9907-c87fc3cc72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f200a-a8fd-4905-a784-4ebcc3064f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2d36d-8e59-4dc4-a157-6602dfb3c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"highlights\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae00c633-e68b-4d71-91cb-3c6103577370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a998358-7d3c-4df4-96af-1ee88725f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534015d-264b-41be-a2f9-586ee18c81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b7c7e-b46c-4714-9c1b-09e2b0498399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ba4f9-10b3-4dec-8840-e794cce68c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5813bd-6d03-4d36-82e3-155b7c39d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dd8ee-51fd-4dd9-8f76-52c937f458aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabdecd0-fe8d-407a-b699-e3a5aa9114dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b75bb3-712e-4b55-b23e-fc393bd4ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, \n",
    "          validation_data=tf_test_set, \n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c010c-d786-4ff9-9c03-e3d78eccb0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
